{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.utils.sp_utils' from '/home/fsamir/gnn/packages/utils/sp_utils.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import packages.utils.sp_utils as sp_utils\n",
    "importlib.reload(sp_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports \n",
    "from packages.data_management.pkl_io import save_pkl, load_pkl_from_path\n",
    "from packages.utils.sp_utils import select_submatrix, convert_scipy_sparse_to_torch\n",
    "from packages.transformer.data import retrieve_features_for_minibatch, retrieve_labels_for_minibatch, TransformerGraphBundleInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit_adj(path=\"data/reddit_cpr\", fname=\"reddit_adj_coo.npz\"):\n",
    "    adj = sp.sparse.load_npz(f'{path}/{fname}')\n",
    "    return adj\n",
    "\n",
    "def load_reddit_feats(path=\"data/reddit_cpr\", fname=\"reddit_feats.npy\"):\n",
    "    with open(f\"{path}/{fname}\", \"rb\") as f:\n",
    "        feats = np.load(f)\n",
    "        return feats\n",
    "\n",
    "def load_reddit_labels(path=\"data/reddit_cpr\", fname=\"reddit_labels.npy\"):\n",
    "    with open(f\"{path}/{fname}\", \"rb\") as f:\n",
    "        labels = np.load(f)\n",
    "        return labels \n",
    "\n",
    "def load_reddit_masks(path=\"data/reddit_cpr\", fname=\"reddit_masks.npy\"):\n",
    "    with open(f\"{path}/{fname}\", \"rb\") as f:\n",
    "        masks = np.load(f)\n",
    "        return masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232965, 232965)\n"
     ]
    }
   ],
   "source": [
    "adj_sparse = load_reddit_adj()\n",
    "print(adj_sparse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232965, 602)\n"
     ]
    }
   ],
   "source": [
    "feats = load_reddit_feats()\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 232965)\n"
     ]
    }
   ],
   "source": [
    "masks = load_reddit_masks()\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232965,)\n"
     ]
    }
   ],
   "source": [
    "labels = load_reddit_labels()\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = np.arange(masks.shape[1])\n",
    "train_ids = all_ids[masks[0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.graph((adj_sparse.row, adj_sparse.col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([5, 5])\n",
    "dataloader = dgl.dataloading.DataLoader(\n",
    "    graph, train_ids, sampler,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_batch(target_nodes, subgraph_nodes, mfg, sparse_adj, features, device):\n",
    "    all_parallel_indices = torch.arange(subgraph_nodes.shape[0], device=device)\n",
    "\n",
    "    src_inds_first_layer = (mfg[0].srcdata[dgl.NID])\n",
    "    dst_inds_first_layer = (mfg[0].dstdata[dgl.NID])\n",
    "    two_hop_neighbour_inds_argsort_inds = all_parallel_indices[dst_inds_first_layer.shape[0]:]\n",
    "    output_node_argsort_inds = all_parallel_indices[: target_nodes.shape[0]] # NOTE: is this an invariant form for DGL? It's probably not guaranteed \n",
    "\n",
    "    first_layer_adj_submatrix = select_submatrix(sparse_adj, src_inds_first_layer, all_parallel_indices, device) # TODO: does this work?\n",
    "    first_layer_adj_submatrix = first_layer_adj_submatrix + torch.eye(first_layer_adj_submatrix.shape[0], device=device) # NOTE: adding self-connections.\n",
    "\n",
    "    second_layer_adj_submatrix = first_layer_adj_submatrix.detach().clone()\n",
    "    second_layer_adj_submatrix[:, two_hop_neighbour_inds_argsort_inds] = 0 \n",
    "    second_layer_adj_submatrix = second_layer_adj_submatrix + torch.eye(second_layer_adj_submatrix.shape[0], device=device) # NOTE: adding self-connections.\n",
    "    \n",
    "    minibatch_adjacencies = torch.stack((first_layer_adj_submatrix, second_layer_adj_submatrix))\n",
    "    all_minibatch_feats = retrieve_features_for_minibatch(src_inds_first_layer, features)\n",
    "\n",
    "    all_minibatch_feats = all_minibatch_feats.unsqueeze(0)\n",
    "    minibatch_adjacencies = minibatch_adjacencies.unsqueeze(0)\n",
    "    minibatch_labels = retrieve_labels_for_minibatch(target_nodes, labels).unsqueeze(0)\n",
    "    output_node_inds = output_node_argsort_inds.unsqueeze(0)\n",
    "\n",
    "    minibatch = TransformerGraphBundleInput(all_minibatch_feats, minibatch_labels, minibatch_adjacencies, output_node_inds)\n",
    "    return minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iter = iter(dataloader)\n",
    "input_nodes, output_nodes, mfgs = next(dataloader_iter) # input nodes gives us the requisite features. The mfgs gives us the requisite attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfg = load_pkl_from_path(\"data/reddit_cpr/mfg\")\n",
    "print(mfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/reddit_cpr/subgraph_nodes.npy\", \"wb\") as f:\n",
    "    f.write(input_nodes.detach().numpy())\n",
    "    \n",
    "with open(\"data/reddit_cpr/target_nodes.npy\", \"wb\") as f:\n",
    "    f.write(output_nodes.detach().numpy())\n",
    "\n",
    "save_pkl(\"mfg\", mfgs, \"data/reddit_cpr/\")\n",
    "\n",
    "\n",
    "# print(input_nodes)\n",
    "# print(mfgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_sparse = convert_scipy_sparse_to_torch(adj_sparse, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "minibatch = construct_batch(output_nodes, input_nodes, mfgs, adj_sparse, feats, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4ebfc88ccd97cb231efe00c7198020b2ae0235bcff1ce31852dbbe06876d933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
